{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the CUDA GPU"
      ],
      "metadata": {
        "id": "CHIIOROylv_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi  # displaying information about available GPUs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3FDErp-MmrR",
        "outputId": "14a42de7-67c6-4f06-ffdc-87e15f4d39ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 24 07:19:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsDCDp1vMnjx",
        "outputId": "e4dcdfaf-039b-4abd-c2da-cbdad8300e80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Collecting cudf-cu11\n",
            "  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-23.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (502.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (5.3.2)\n",
            "Collecting cubinlinker-cu11 (from cudf-cu11)\n",
            "  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cuda-python<12.0a0,>=11.7.1 (from cudf-cu11)\n",
            "  Downloading cuda_python-11.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cupy-cuda11x>=12.0.0 (from cudf-cu11)\n",
            "  Downloading cupy_cuda11x-12.2.0-cp310-cp310-manylinux2014_x86_64.whl (89.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (2023.6.0)\n",
            "Collecting numba<0.58,>=0.57 (from cudf-cu11)\n",
            "  Downloading numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.25,>=1.21 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (1.23.5)\n",
            "Collecting nvtx>=0.2.1 (from cudf-cu11)\n",
            "  Downloading nvtx-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (582 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.4/582.4 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (23.2)\n",
            "Requirement already satisfied: pandas<1.6.0dev0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (1.5.3)\n",
            "Collecting protobuf<5,>=4.21 (from cudf-cu11)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ptxcompiler-cu11 (from cudf-cu11)\n",
            "  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.7.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==12.* (from cudf-cu11)\n",
            "  Downloading pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (13.7.0)\n",
            "Collecting rmm-cu11==23.10.* (from cudf-cu11)\n",
            "  Downloading https://pypi.nvidia.com/rmm-cu11/rmm_cu11-23.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu11) (4.5.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x>=12.0.0->cudf-cu11) (0.8.2)\n",
            "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba<0.58,>=0.57->cudf-cu11)\n",
            "  Downloading llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu11) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu11) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu11) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.3->cudf-cu11) (1.16.0)\n",
            "Installing collected packages: ptxcompiler-cu11, nvtx, cuda-python, cubinlinker-cu11, pyarrow, protobuf, llvmlite, cupy-cuda11x, numba, rmm-cu11, cudf-cu11\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.41.1\n",
            "    Uninstalling llvmlite-0.41.1:\n",
            "      Successfully uninstalled llvmlite-0.41.1\n",
            "  Attempting uninstall: cupy-cuda11x\n",
            "    Found existing installation: cupy-cuda11x 11.0.0\n",
            "    Uninstalling cupy-cuda11x-11.0.0:\n",
            "      Successfully uninstalled cupy-cuda11x-11.0.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.58.1\n",
            "    Uninstalling numba-0.58.1:\n",
            "      Successfully uninstalled numba-0.58.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 12.0.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cubinlinker-cu11-0.3.0.post1 cuda-python-11.8.3 cudf-cu11-23.10.2 cupy-cuda11x-12.2.0 llvmlite-0.40.1 numba-0.57.1 nvtx-0.2.8 protobuf-4.25.1 ptxcompiler-cu11-0.7.0.post1 pyarrow-12.0.1 rmm-cu11-23.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cudf"
      ],
      "metadata": {
        "id": "dFoEcZxrMpty"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "ynWS6YhyltXX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dNzZ0T0Gb677"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the activation functions"
      ],
      "metadata": {
        "id": "1N_Q4Ay2lqS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "05Ssf8D4b678"
      },
      "outputs": [],
      "source": [
        "# Implement the following activation functions (along with their gradient functions): sigmoid, tanh, ReLU, Leaky ReLU, linear, and softmax\n",
        "\n",
        "# Sigmoid Function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "\n",
        "# Tanh Function\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "\n",
        "# ReLU Function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "# Leaky ReLU Function\n",
        "def leaky_relu(x):\n",
        "    return np.where(x > 0, x, x * 0.01)\n",
        "\n",
        "def leaky_relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0.01)\n",
        "\n",
        "\n",
        "# Linear Function\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def linear_derivative(x):\n",
        "    return 1\n",
        "\n",
        "\n",
        "# Softmax Function\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    return softmax(x) * (1 - softmax(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the weight initialization functions"
      ],
      "metadata": {
        "id": "6F6AyiQVlmI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZbZtZBQvb678"
      },
      "outputs": [],
      "source": [
        "# Implement the following weight initialization functions: zero init, random init, and normal init (Normal(0, 1)). Choose appropriate scaling factors.\n",
        "\n",
        "# Zero Initialization\n",
        "def zero_init(shape):\n",
        "    return np.zeros(shape)\n",
        "\n",
        "# Random Initialization\n",
        "def random_init(shape):\n",
        "    return np.random.rand(shape[0], shape[1])\n",
        "\n",
        "# Normal Initialization\n",
        "def normal_init(shape):\n",
        "    return np.random.normal(0, 1, (shape[0], shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a class named NeuralNetwork with the following parameters during initialization:\n",
        " • N: Number of layers in the network\n",
        "• A list of size N specifying the number of neurons in each layer\n",
        "• lr: Learning rate\n",
        "• Activation function (same activation function is used in all layers of the network except the last layer)\n",
        "• Weight initialization function\n",
        "• Number of epochs"
      ],
      "metadata": {
        "id": "n4SV_924kiUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Neural Network Class"
      ],
      "metadata": {
        "id": "B6aOiwh9lhkD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hYAb_PXsb678"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, N, A, lr, activation, weight_init, epochs, batch_size):\n",
        "        self.layers = N + 2\n",
        "        self.lr = lr\n",
        "        self.activation = activation\n",
        "        self.weight_init = weight_init\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # Creating a list of neurons in each hidden layer\n",
        "        self.neurons = A.copy()\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initializing the weights and biases\n",
        "        for i in range(self.layers - 1):\n",
        "            self.weights.append(self.initialize_weight((self.neurons[i + 1], self.neurons[i])))\n",
        "            # self.biases.append(self.initialize_weight((self.neurons[i + 1], 1)))\n",
        "\n",
        "        # Creating a dictionary for A and Z\n",
        "        self.A = {}\n",
        "        self.Z = {}\n",
        "\n",
        "    def initialize_weight(self, shape):\n",
        "        if self.weight_init == 'zero':\n",
        "            return zero_init(shape)\n",
        "        elif self.weight_init == 'random':\n",
        "            return random_init(shape)\n",
        "        elif self.weight_init == 'normal':\n",
        "            return normal_init(shape)\n",
        "        else:\n",
        "            return np.random.randn(shape[1], shape[0]) * np.sqrt(1.0 / shape[1])\n",
        "\n",
        "\n",
        "    def __activation_(self, x, derivative=False):\n",
        "        if self.activation == 'sigmoid':\n",
        "            if derivative:\n",
        "                return sigmoid_derivative(x)\n",
        "            else:\n",
        "                return sigmoid(x)\n",
        "        elif self.activation == 'tanh':\n",
        "            if derivative:\n",
        "                return tanh_derivative(x)\n",
        "            else:\n",
        "                return tanh(x)\n",
        "        elif self.activation == 'relu':\n",
        "            if derivative:\n",
        "                return relu_derivative(x)\n",
        "            else:\n",
        "                return relu(x)\n",
        "        elif self.activation == 'leaky_relu':\n",
        "            if derivative:\n",
        "                return leaky_relu_derivative(x)\n",
        "            else:\n",
        "                return leaky_relu(x)\n",
        "        elif self.activation == 'linear':\n",
        "            if derivative:\n",
        "                return linear_derivative(x)\n",
        "            else:\n",
        "                return linear(x)\n",
        "        elif self.activation == 'softmax':\n",
        "            if derivative:\n",
        "                return softmax_derivative(x)\n",
        "            else:\n",
        "                return softmax(x)\n",
        "\n",
        "    def forward_prop(self, X_train):\n",
        "        self.A[0] = X_train\n",
        "        for i in range(self.layers - 2):\n",
        "            self.Z[i + 1] = np.dot(self.weights[i], self.A[i])\n",
        "            self.A[i + 1] = self.__activation_(self.Z[i + 1])\n",
        "\n",
        "        # Z4 & A4 completed. Calculating Z5 & A5\n",
        "        self.Z[self.layers-1] = np.dot(self.weights[self.layers - 2], self.A[self.layers - 2])\n",
        "        self.A[self.layers-1] = softmax(self.Z[self.layers-1])\n",
        "\n",
        "        return self.A[self.layers-1]\n",
        "\n",
        "    def backward_prop(self, y_train, y_pred):\n",
        "        # Calculating the gradients of the last layer\n",
        "        # Calculating error\n",
        "        dW = [0]*(self.layers - 1)\n",
        "        dB = [0]*(self.layers - 1)\n",
        "        error = y_pred - y_train\n",
        "\n",
        "        de = 2 * error / y_pred.shape[0] * softmax_derivative(self.Z[self.layers-1])\n",
        "\n",
        "        dW[self.layers - 2] = np.outer(de, self.A[self.layers - 2])\n",
        "\n",
        "        # Calculating dW and dB for the rest of the layers\n",
        "        for i in range(self.layers - 2, 0, -1):\n",
        "            de = np.dot(self.weights[i].T, de) * self.__activation_(self.Z[i], derivative=True)\n",
        "            dW[i - 1] = np.outer(de, self.A[i - 1])\n",
        "\n",
        "        # Updating the weights and biases\n",
        "        for i in range(self.layers - 1):\n",
        "            self.weights[i] -= self.lr * dW[i]\n",
        "\n",
        "\n",
        "    def __scale_data(self, X_train):\n",
        "        # Scaling the data after each epoch for the MNIST dataset\n",
        "        X_train = X_train / ((np.max(X_train) - np.min(X_train))*0.99) + 0.01\n",
        "        return X_train\n",
        "\n",
        "\n",
        "    def fit(self, X_train, y_train, X_val, y_val, early_stopping=False, tol=0.0001):\n",
        "        # Training loss and validation loss\n",
        "        train_loss = []\n",
        "        val_loss = []\n",
        "\n",
        "        # Scaling the data\n",
        "        X_train = self.__scale_data(X_train)\n",
        "        X_val = self.__scale_data(X_val)\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            for j in range(0, X_train.shape[0], self.batch_size):\n",
        "\n",
        "                X_train_mini = X_train[j:j + self.batch_size].T\n",
        "                y_train_mini = y_train[j:j + self.batch_size].T\n",
        "                # Transforming the y_train_mini to a one-hot vector\n",
        "                output_neuron_y = []\n",
        "                for sample in y_train_mini:\n",
        "                    temp = np.zeros(10) + 0.01\n",
        "                    temp[sample] = 0.99\n",
        "                    output_neuron_y.append(temp)\n",
        "                output_neuron_y = np.array(output_neuron_y).T\n",
        "\n",
        "                y_pred = self.forward_prop(X_train_mini)\n",
        "                self.backward_prop(output_neuron_y, y_pred)\n",
        "\n",
        "            # Print the accuracy after each epoch\n",
        "            y_temp_pred = self.predict(X_train)\n",
        "            y_temp_val_pred = self.predict(X_val)\n",
        "\n",
        "            # Calculating the training loss\n",
        "            train_loss.append(self.loss(y_train, y_temp_pred))\n",
        "\n",
        "            # Calculating the validation loss\n",
        "            val_loss.append(self.loss(y_val, y_temp_val_pred))\n",
        "\n",
        "            print('Epoch', i, 'Validation Accuracy:', self.accuracy(y_val, y_temp_val_pred), 'Training Accuracy:', self.accuracy(y_train, y_temp_pred))\n",
        "            print('Epoch', i, 'Validation Loss:', val_loss[i], 'Training Loss:', train_loss[i])\n",
        "            print('--------------------------------------------------')\n",
        "\n",
        "            # Early stopping\n",
        "            if early_stopping:\n",
        "                if i > 0:\n",
        "                    if abs(val_loss[i] - val_loss[i - 1]) < tol:\n",
        "                        print('Early Stopping')\n",
        "                        break\n",
        "\n",
        "        return train_loss, val_loss\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        y_pred = []\n",
        "        for sample in X_test:\n",
        "            y_pred.append(np.argmax(self.forward_prop(sample)))\n",
        "        return y_pred\n",
        "\n",
        "    def predict_proba(self, X_test):\n",
        "        y_pred = self.forward_prop(X_test)\n",
        "        return y_pred\n",
        "\n",
        "    def score(self, X_test, y_test):\n",
        "        y_pred = self.predict(X_test)\n",
        "        return np.mean(y_pred == y_test)\n",
        "\n",
        "    def loss(self, y_train, y_pred):\n",
        "        loss = np.sum(-y_train * np.log(y_pred))\n",
        "        return loss\n",
        "\n",
        "    def accuracy(self, y_test, y_pred):\n",
        "        p = []\n",
        "        for i in range(len(y_pred)):\n",
        "            out_test = np.zeros(10) + 0.01\n",
        "            out_test[y_test[i]] = 0.99\n",
        "            p.append(y_pred[i] == np.argmax(out_test))\n",
        "\n",
        "        return np.mean(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BoFJ8TwMXkb"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL1NCbYLMXkc",
        "outputId": "ab3ab935-72e9-4c93-b76a-5a60794aaf23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKrm94GOMXkd"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eITg96_8MXka"
      },
      "outputs": [],
      "source": [
        "NN = NeuralNetwork(N=2, A=[784, 256, 32, 10], lr=0.001, activation='sigmoid', weight_init='normal', epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FYzrpyp-DpG2",
        "outputId": "c655bd07-dfae-43e9-e232-915f96994a95"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n",
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Validation Accuracy: 0.1229 Training Accuracy: 0.12261666666666667\n",
            "Epoch 0 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 1 Validation Accuracy: 0.1534 Training Accuracy: 0.15058333333333335\n",
            "Epoch 1 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 2 Validation Accuracy: 0.1767 Training Accuracy: 0.16893333333333332\n",
            "Epoch 2 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 3 Validation Accuracy: 0.1886 Training Accuracy: 0.18261666666666668\n",
            "Epoch 3 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 4 Validation Accuracy: 0.2043 Training Accuracy: 0.1968\n",
            "Epoch 4 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 5 Validation Accuracy: 0.2202 Training Accuracy: 0.20891666666666667\n",
            "Epoch 5 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 6 Validation Accuracy: 0.2344 Training Accuracy: 0.22373333333333334\n",
            "Epoch 6 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 7 Validation Accuracy: 0.2507 Training Accuracy: 0.23965\n",
            "Epoch 7 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 8 Validation Accuracy: 0.2641 Training Accuracy: 0.25483333333333336\n",
            "Epoch 8 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 9 Validation Accuracy: 0.2761 Training Accuracy: 0.2698833333333333\n",
            "Epoch 9 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 10 Validation Accuracy: 0.2886 Training Accuracy: 0.2835666666666667\n",
            "Epoch 10 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 11 Validation Accuracy: 0.2972 Training Accuracy: 0.29433333333333334\n",
            "Epoch 11 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 12 Validation Accuracy: 0.3068 Training Accuracy: 0.304\n",
            "Epoch 12 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 13 Validation Accuracy: 0.317 Training Accuracy: 0.31445\n",
            "Epoch 13 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 14 Validation Accuracy: 0.3267 Training Accuracy: 0.32425\n",
            "Epoch 14 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 15 Validation Accuracy: 0.3359 Training Accuracy: 0.3337\n",
            "Epoch 15 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 16 Validation Accuracy: 0.3437 Training Accuracy: 0.34341666666666665\n",
            "Epoch 16 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 17 Validation Accuracy: 0.3515 Training Accuracy: 0.35185\n",
            "Epoch 17 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 18 Validation Accuracy: 0.3605 Training Accuracy: 0.36088333333333333\n",
            "Epoch 18 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 19 Validation Accuracy: 0.3704 Training Accuracy: 0.36911666666666665\n",
            "Epoch 19 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 20 Validation Accuracy: 0.3782 Training Accuracy: 0.3783\n",
            "Epoch 20 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 21 Validation Accuracy: 0.3868 Training Accuracy: 0.38561666666666666\n",
            "Epoch 21 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 22 Validation Accuracy: 0.3948 Training Accuracy: 0.39335\n",
            "Epoch 22 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 23 Validation Accuracy: 0.4023 Training Accuracy: 0.40145\n",
            "Epoch 23 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 24 Validation Accuracy: 0.4085 Training Accuracy: 0.40925\n",
            "Epoch 24 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 25 Validation Accuracy: 0.4153 Training Accuracy: 0.4166166666666667\n",
            "Epoch 25 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 26 Validation Accuracy: 0.4227 Training Accuracy: 0.42391666666666666\n",
            "Epoch 26 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 27 Validation Accuracy: 0.4312 Training Accuracy: 0.43151666666666666\n",
            "Epoch 27 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 28 Validation Accuracy: 0.4395 Training Accuracy: 0.4402333333333333\n",
            "Epoch 28 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 29 Validation Accuracy: 0.4491 Training Accuracy: 0.44761666666666666\n",
            "Epoch 29 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 30 Validation Accuracy: 0.4571 Training Accuracy: 0.4557\n",
            "Epoch 30 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 31 Validation Accuracy: 0.464 Training Accuracy: 0.46335\n",
            "Epoch 31 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 32 Validation Accuracy: 0.4723 Training Accuracy: 0.4713333333333333\n",
            "Epoch 32 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 33 Validation Accuracy: 0.4797 Training Accuracy: 0.47855\n",
            "Epoch 33 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 34 Validation Accuracy: 0.4882 Training Accuracy: 0.48575\n",
            "Epoch 34 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 35 Validation Accuracy: 0.497 Training Accuracy: 0.4931\n",
            "Epoch 35 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 36 Validation Accuracy: 0.5046 Training Accuracy: 0.49993333333333334\n",
            "Epoch 36 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 37 Validation Accuracy: 0.5124 Training Accuracy: 0.5072333333333333\n",
            "Epoch 37 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 38 Validation Accuracy: 0.5211 Training Accuracy: 0.5138\n",
            "Epoch 38 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 39 Validation Accuracy: 0.526 Training Accuracy: 0.5200666666666667\n",
            "Epoch 39 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 40 Validation Accuracy: 0.5311 Training Accuracy: 0.5265666666666666\n",
            "Epoch 40 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 41 Validation Accuracy: 0.5374 Training Accuracy: 0.5329666666666667\n",
            "Epoch 41 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 42 Validation Accuracy: 0.5437 Training Accuracy: 0.53865\n",
            "Epoch 42 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 43 Validation Accuracy: 0.5507 Training Accuracy: 0.5442333333333333\n",
            "Epoch 43 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 44 Validation Accuracy: 0.5559 Training Accuracy: 0.5495\n",
            "Epoch 44 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 45 Validation Accuracy: 0.5601 Training Accuracy: 0.5544\n",
            "Epoch 45 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 46 Validation Accuracy: 0.5634 Training Accuracy: 0.5589833333333334\n",
            "Epoch 46 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 47 Validation Accuracy: 0.5676 Training Accuracy: 0.5639333333333333\n",
            "Epoch 47 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 48 Validation Accuracy: 0.5724 Training Accuracy: 0.5680666666666667\n",
            "Epoch 48 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 49 Validation Accuracy: 0.5779 Training Accuracy: 0.5718333333333333\n",
            "Epoch 49 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 50 Validation Accuracy: 0.5831 Training Accuracy: 0.57635\n",
            "Epoch 50 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 51 Validation Accuracy: 0.5873 Training Accuracy: 0.58025\n",
            "Epoch 51 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 52 Validation Accuracy: 0.5918 Training Accuracy: 0.5845\n",
            "Epoch 52 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 53 Validation Accuracy: 0.5952 Training Accuracy: 0.5883166666666667\n",
            "Epoch 53 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-beba8b749d5b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val, early_stopping, tol)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0moutput_neuron_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train_mini\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                     \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0moutput_neuron_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = NN.fit(X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv7wgwV3MXke"
      },
      "source": [
        "### Plot Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5q5rDk8NMXke"
      },
      "outputs": [],
      "source": [
        "def plot_loss(train_loss, val_loss):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.plot(train_loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zgyNTwxU5gm"
      },
      "source": [
        "### SigMoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FBx768DgU8YB"
      },
      "outputs": [],
      "source": [
        "NN = NeuralNetwork(N=4, A=[784, 256,128,64,32, 10], lr=0.001, activation='sigmoid', weight_init='normal', epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9zRAzYBvb679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "2c6953b4-0d03-41b4-cff6-bbf5cf9f071e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n",
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Validation Accuracy: 0.0958 Training Accuracy: 0.10073333333333333\n",
            "Epoch 0 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 1 Validation Accuracy: 0.1084 Training Accuracy: 0.11296666666666667\n",
            "Epoch 1 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 2 Validation Accuracy: 0.1296 Training Accuracy: 0.1315\n",
            "Epoch 2 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 3 Validation Accuracy: 0.1555 Training Accuracy: 0.15928333333333333\n",
            "Epoch 3 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 4 Validation Accuracy: 0.182 Training Accuracy: 0.1863\n",
            "Epoch 4 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 5 Validation Accuracy: 0.2026 Training Accuracy: 0.20351666666666668\n",
            "Epoch 5 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 6 Validation Accuracy: 0.2175 Training Accuracy: 0.21875\n",
            "Epoch 6 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 7 Validation Accuracy: 0.2301 Training Accuracy: 0.23316666666666666\n",
            "Epoch 7 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 8 Validation Accuracy: 0.2446 Training Accuracy: 0.2461\n",
            "Epoch 8 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 9 Validation Accuracy: 0.2559 Training Accuracy: 0.2581333333333333\n",
            "Epoch 9 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 10 Validation Accuracy: 0.2701 Training Accuracy: 0.27086666666666664\n",
            "Epoch 10 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e4c5bb6b54e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val, early_stopping, tol)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     \u001b[0moutput_neuron_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0moutput_neuron_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_neuron_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loss, val_loss = NN.fit(X_train, y_train, X_test, y_test, early_stopping=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "G2YkukkHMXkf",
        "outputId": "f5d6bea8-5231-40ed-d958-b2775457905b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-484b807772bb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
          ]
        }
      ],
      "source": [
        "plot_loss(train_loss, val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBgELnfRVQHz"
      },
      "source": [
        "### TanH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "63WII7eHVTDZ"
      },
      "outputs": [],
      "source": [
        "NN = NeuralNetwork(N=4, A=[784, 256,128,64,32, 10], lr=0.001, activation='tanh', weight_init='normal', epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = NN.fit(X_train, y_train, X_test, y_test, early_stopping=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "g2hkqn3FftN8",
        "outputId": "6a26f5e9-702b-42d5-bc57-37777d041276"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n",
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Validation Accuracy: 0.1262 Training Accuracy: 0.13385\n",
            "Epoch 0 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 1 Validation Accuracy: 0.1446 Training Accuracy: 0.14825\n",
            "Epoch 1 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 2 Validation Accuracy: 0.1485 Training Accuracy: 0.15453333333333333\n",
            "Epoch 2 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 3 Validation Accuracy: 0.1471 Training Accuracy: 0.15221666666666667\n",
            "Epoch 3 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 4 Validation Accuracy: 0.1426 Training Accuracy: 0.14991666666666667\n",
            "Epoch 4 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e4c5bb6b54e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val, early_stopping, tol)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m# print(output_neuron_y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0;31m# out_neuron_y = np.zeros(10) + 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# out_neuron_y[y_train[j]] = 0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(self, X_train)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__activation_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn4t31FVMXkf"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss, val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrXWFabcVqLC"
      },
      "source": [
        "### Leaky Relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tASsJjQgVr-E"
      },
      "outputs": [],
      "source": [
        "NN = NeuralNetwork(N=4, A=[784, 256,128,64,32, 10], lr=0.001, activation='leaky_relu', weight_init='normal', epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = NN.fit(X_train, y_train, X_test, y_test, early_stopping=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "ClKW3FfNfwNP",
        "outputId": "fb9c9de9-ac91-4b1b-e56b-65626c6cbdf9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-57d3f8a8ec65>:45: RuntimeWarning: overflow encountered in exp\n",
            "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
            "<ipython-input-5-57d3f8a8ec65>:45: RuntimeWarning: invalid value encountered in divide\n",
            "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n",
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 0 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 1 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 1 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 2 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 2 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 3 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 3 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 4 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 4 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 5 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 5 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e4c5bb6b54e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val, early_stopping, tol)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m# print(output_neuron_y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0;31m# out_neuron_y = np.zeros(10) + 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;31m# out_neuron_y[y_train[j]] = 0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(self, X_train)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__activation_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53oLQdLcMXkg"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss, val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ufLj4SrVw0i"
      },
      "source": [
        "### Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8LNQIXCNVyIq"
      },
      "outputs": [],
      "source": [
        "NN = NeuralNetwork(N=4, A=[784, 256,128,64,32, 10], lr=0.001, activation='linear', weight_init='normal', epochs=100, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss = NN.fit(X_train, y_train, X_test, y_test, early_stopping=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "Gx477f7ff2F7",
        "outputId": "ef11edb6-b601-486f-ffc4-8a982813d4fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-57d3f8a8ec65>:45: RuntimeWarning: overflow encountered in exp\n",
            "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
            "<ipython-input-5-57d3f8a8ec65>:45: RuntimeWarning: invalid value encountered in divide\n",
            "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n",
            "<ipython-input-7-2af6ec768c0e>:213: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = np.sum(-y_train * np.log(y_pred))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 0 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 1 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 1 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 2 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 2 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 3 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 3 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 4 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 4 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n",
            "Epoch 5 Validation Accuracy: 0.098 Training Accuracy: 0.09871666666666666\n",
            "Epoch 5 Validation Loss: nan Training Loss: nan\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e4c5bb6b54e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, X_val, y_val, early_stopping, tol)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# print(out_neuron_y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_neuron_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;31m# print('Completed 1 batch')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# print('Weights:', self.weights[0][0:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2af6ec768c0e>\u001b[0m in \u001b[0;36mbackward_prop\u001b[0;34m(self, y_train, y_pred)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Updating weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;31m# print('Updation: ', self.lr * dW[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;31m# print('After weight: ', self.weights[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# self.biases[i] -= self.lr * dB[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEzWS2yDMXkg"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss, val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7-Bb3sRMXkg"
      },
      "source": [
        "### Plotting first 10 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QZQs7J7RMXkg"
      },
      "outputs": [],
      "source": [
        "def plot_first_10_images(X_train, y_train):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(10):\n",
        "        plt.subplot(1, 10, i + 1)\n",
        "        plt.imshow(X_train[i], cmap='gray')\n",
        "        plt.title(y_train[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mhCGPyaipW_r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "a62ea213-525f-486d-a92c-ce72fba62bb7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "plot_first_10_images(X_train, y_train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}